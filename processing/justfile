set dotenv-load

# Show help
default: help
help:
    @just --list --unsorted

# ==== variables ====
PROJECT_ROOT := `dirname $(pwd)`
PY_FILES := `fd --extension py --type file .  | tr '\n' ' '`

# ==== codebase ====

# sanity
[group('codebase')]
sanity:
    #!/bin/bash
    echo "PROJECT_ROOT: {{PROJECT_ROOT}}"
    echo "PY_FILES: {{PY_FILES}}"
    echo "\$ACCOUNT_CODE: ${ACCOUNT_CODE}"
    uv run scripts/sanity.py

# ruff format and ruff check
[group('codebase')]
ruff:
    @echo "{{PY_FILES}}" | xargs uv run ruff format
    @echo "{{PY_FILES}}" | xargs uv run ruff check --fix

# Lint using ty check
[group('codebase')]
ty:
    uv run ty check

# run all quality checks
[group('codebase')]
check: ruff ty

# ==== main-processing ====

# Preprocess trait data for embedding generation
[group('main-processing')]
preprocess-traits:
    uv run scripts/main-processing/preprocess-traits.py

# Preprocess EFO ontology data for embedding generation
[group('main-processing')]
preprocess-efo:
    uv run scripts/main-processing/preprocess-efo.py

# Submit HPC batch job to generate trait embeddings
[group('main-processing'), group('hpc')]
embed-traits:
    sbatch --account=${ACCOUNT_CODE} scripts/bc4/embed-traits.sbatch

# Submit HPC batch job to generate EFO embeddings
[group('main-processing'), group('hpc')]
embed-efo:
    sbatch --account=${ACCOUNT_CODE} scripts/bc4/embed-efo.sbatch

# Aggregate embedding results from HPC batch jobs
[group('main-processing')]
aggregate-embeddings:
    #!/bin/bash
    # NOTE: note about the hardcoded path with hpc experiements
    TRAIT_EXPERIMENT_ID="bc4-12790155"
    EFO_EXPERIMENT_ID="bc4-12432782"
    uv run scripts/main-processing/aggregate-embeddings.py \
        --trait-results-dir {{PROJECT_ROOT}}/data/output/${TRAIT_EXPERIMENT_ID}/results \
        --efo-results-dir {{PROJECT_ROOT}}/data/output/${EFO_EXPERIMENT_ID}/results

# ==== main database ====

# Build main vector store database with embeddings and model results
[group('main-database')]
build-main-db:
    uv run scripts/main-db/build-main-database.py \
        -db vector_store \
        --force-write

# ==== trait profile similarity database ====

# Submit HPC batch job to compute pairwise trait similarities
[group('trait-profile'), group('hpc')]
compute-trait-similarities:
    sbatch --account=${ACCOUNT_CODE} scripts/bc4/compute-trait-similarity.sbatch

# Aggregate trait similarity results from HPC batch jobs
[group('trait-profile')]
aggregate-trait-similarities:
    #!/bin/bash
    # NOTE: note about the hardcoded path with hpc experiement
    EXPERIMENT_ID="bc4-12468629"
    uv run scripts/trait-profile/aggregate-trait-similarities.py \
        --input-dir {{PROJECT_ROOT}}/data/output/${EXPERIMENT_ID}/results

# Build trait profile database with similarity data
[group('trait-profile')]
build-trait-profile-db:
    uv run scripts/trait-profile/build-trait-profile-database.py \
        -db trait_profile_db \
        --memory-limit 8GB \
        --force-write

# ==== postprocessing ====

# Generate database schema documentation
[group('postprocessing')]
generate-schema-docs:
    uv run python scripts/postprocessing/generate-schema-docs.py
